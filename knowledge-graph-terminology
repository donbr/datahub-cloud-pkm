# Glossary of Knowledge Graph Terms (2025)

## Centrality  
**Centrality** is a measure of the importance or influence of a node within a graph or network ([Graph Theory and Network Analysis in Data Science](https://www.careerera.com/blog/graph-theory-and-network-analysis-in-data-science#:~:text=1)). Different types of centrality capture different aspects of a node’s prominence – for example, **degree centrality** counts how many direct connections a node has, while **betweenness centrality** measures how often a node lies on shortest paths between other nodes. High centrality indicates a node that plays a key role in the network’s structure. *For example, in a social network, a person with many connections (high degree centrality) may be an influencer because they can reach others easily.* 

## Community Detection  
**Community Detection** refers to the process of identifying groups of closely related nodes (communities) within a graph. These algorithms partition a network into subgroups of nodes that are more densely connected to each other than to the rest of the network ([Top Community Detection Algorithms Compared - Dgraph Blog](https://dgraph.io/blog/post/community-detection-algorithms/#:~:text=Community%20detection%20algorithms%20identify%20groups,to%20nodes%20in%20different%20communities)). Finding such communities helps reveal cluster structures – for instance, groups of customers with similar purchase patterns or friend circles in a social network. Community detection provides insight into the hidden structure of complex networks, enabling targeted analysis of each subgroup. 

## Cypher  
**Cypher** is a declarative query language for property graph databases, originally developed by Neo4j. It uses a pattern-matching syntax (inspired by SQL) for querying and updating graph data ([Neo4j vs MongoDB - What's the Difference ? (Pros and Cons)](https://cloudinfrastructureservices.co.uk/neo4j-vs-mongodb-whats-the-difference/#:~:text=Moreover%2C%20it%20also%20provides%20a,queries%20as%20pattern%20matching%20statements)). With Cypher, users describe graph patterns to retrieve data without specifying procedural traversal steps – for example, a Cypher query can find all friends-of-friends by matching a path `()-[:KNOWS]-()-[:KNOWS]->()`. Cypher has become one of the most widely adopted query languages for property graphs and is available as **openCypher**, an open standard, across multiple graph database systems ([The Complete Cypher Cheat Sheet](https://memgraph.com/blog/cypher-cheat-sheet#:~:text=Cypher%20is%20the%20most%20widely,to%20work%20with%20property%20graphs)). Its easy-to-read, SQL-like style makes it accessible for developers and analysts working with graph data. 

## Data Stewardship  
**Data Stewardship** involves the day-to-day management, curation, and oversight of an organization’s data assets to ensure they are high-quality, well-documented, and used appropriately ([Data Governance vs Data Stewardship: 5 Key Differences](https://atlan.com/data-governance-vs-data-stewardship/#:~:text=,with%20governance%20policies%20and%20procedures)). A data steward’s responsibilities include enforcing data standards and policies, handling data issues (such as correcting errors or resolving ambiguities), and controlling access to data. In the context of a knowledge graph, data stewardship ensures that ontologies, taxonomies, and data entries remain consistent and reliable over time. This practice is a critical part of data governance – whereas data governance sets the policies and standards, data stewardship is the operational execution that keeps the data trustworthy and compliant with those policies. 

## Data Virtualization  
**Data Virtualization** is an approach to data integration that provides a unified, real-time view of data from disparate sources without physically moving or copying the data. It creates a virtual data access layer that lets applications retrieve and manipulate data without needing to know the technical details of each source (such as format or location) ([Data virtualization - Wikipedia](https://en.wikipedia.org/wiki/Data_virtualization#:~:text=Data%20virtualization%20is%20an%20approach,2)). Unlike traditional ETL processes, in data virtualization the data remains in place at the source systems, and queries are federated across those sources on the fly ([Data virtualization - Wikipedia](https://en.wikipedia.org/wiki/Data_virtualization#:~:text=Unlike%20the%20traditional%20extract%2C%20transform%2C,oriented)). This reduces duplication and latency, enabling a “single view” of enterprise data for analytics and reporting. *For example, a virtualized data layer might let a business intelligence dashboard join customer information from a CRM database with purchase data from a data warehouse in real time.* 

## Entity Resolution  
**Entity Resolution** is the process of identifying and merging records that refer to the same real-world entity across different data sources or datasets. It reconciles separate representations of an entity to create one unified, consistent profile ([Entity Resolution – Diffblog](https://blog.diffbot.com/knowledge-graph-glossary/entity-resolution/#:~:text=Entity%20Resolution%20refers%20to%20a,to%20facts%20about%20an%20entity)). This often involves matching on identifiers or attributes and disambiguating similar or duplicate records. Effective entity resolution tackles challenges like variant spellings of names, duplicate customer records, or differing codes for the same product. *For example, the records “IBM” and “International Business Machines Corp.” might be resolved and linked to a single entity in a knowledge graph.* By resolving entities, an enterprise knowledge graph can eliminate duplicate nodes and ensure that each real-world object is represented once, with all relevant facts consolidated. 

## ETL (Extract, Transform, Load)  
**ETL** stands for *Extract, Transform, Load*, a traditional process for consolidating data from multiple sources into a target system or data store. In the ETL process, data is first **extracted** from source systems, then **transformed** – e.g. cleaned, formatted, or integrated (applying business rules to ensure consistency) – and finally **loaded** into a destination such as a data warehouse or knowledge graph repository ([What is ETL (Extract, Transform, Load)? | IBM](https://www.ibm.com/think/topics/etl#:~:text=ETL%E2%80%94meaning%20extract%2C%20transform%2C%20load%E2%80%94is%20a,lake%20or%20other%20target%20system)). ETL pipelines are fundamental in building enterprise knowledge graphs: they might pull data from relational databases, CSV files, and APIs, standardize it (for example, reconciling different date formats or merging duplicate entries), and then load it into a graph database or triple store. The ETL process ensures that the integrated data is consistent and ready for querying, but it typically involves batch processing, in contrast to real-time integration approaches. 

## Gremlin  
**Gremlin** is a graph traversal language and query framework developed as part of Apache TinkerPop. It allows users to navigate, filter, and compute on graph data through a sequence of steps (traversals) that can be written in a variety of programming languages. Gremlin is often described as a *graph traversal language and virtual machine* for property graphs ([Gremlin (query language) - Wikipedia](https://en.wikipedia.org/wiki/Gremlin_(query_language)#:~:text=Gremlin%20is%20a%20graph%20traversal,functional%20language%20foundation%20enable%20Gremlin)). It works across both OLTP graph databases (for real-time queries) and OLAP graph processors (for analytical workloads) by enabling complex traversals of the graph structure. For example, a Gremlin traversal can start at a particular vertex, follow outgoing edges to neighboring vertices, then apply transformations or aggregations (such as calculating the average age of connected people). Gremlin’s flexible, step-by-step approach makes it possible to express intricate graph algorithms (like finding all paths, subgraph pattern matching, centrality calculations, etc.) in a database-agnostic way – many graph systems (Amazon Neptune, Azure Cosmos DB, JanusGraph, etc.) support Gremlin as a query language. 

## Knowledge Graph  
**Knowledge Graph** is a highly connected data structure (or knowledge base) that represents information as a network of entities (nodes) and the relationships (edges) between them. It serves as a flexible, semantic data layer that integrates information from various sources, enabling complex queries and inference across data silos ([What is a Knowledge Graph | Stardog](https://www.stardog.com/knowledge-graph/#:~:text=A%20Knowledge%20Graph%20is%20a,Knowledge%20Graph%20of%20enterprise%20data)). In a knowledge graph, each node typically represents a real-world concept or object (like a person, place, product, or term) and edges capture the semantic relationships between those concepts (e.g. *“Alice worksFor CompanyX”* or *“Paris isCapitalOf France”*). The graph is usually backed by an ontology or schema that defines the types of entities and relationships, allowing the data to be machine-understandable. Knowledge graphs are designed to capture context and meaning, making it easier to answer complex questions (for instance, “Which employees of CompanyX have expertise in Graph Databases?” can be answered by traversing a knowledge graph). They easily accommodate new data and schema changes, which is crucial for evolving business needs ([What is a Knowledge Graph | Stardog](https://www.stardog.com/knowledge-graph/#:~:text=A%20Knowledge%20Graph%20is%20a,Knowledge%20Graph%20of%20enterprise%20data)). *A well-known example is Google’s Knowledge Graph, which aggregates facts about people, places, and things and powers the informational panels in Google search results.* In an enterprise setting, a knowledge graph might unify customer data, product information, and industry knowledge into a single connected model to support advanced analytics and decision-making. 

## Ontology  
**Ontology** (in the context of knowledge graphs and semantic data) is a formal description of the concepts, relationships, and constraints that define a certain domain of knowledge. In essence, an ontology serves as a schema or blueprint for a knowledge graph, specifying the classes (entity types), properties (attributes and relationships), and rules that apply in the domain ([What is an Ontology?](https://www.oxfordsemantic.tech/faqs/what-is-an-ontology#:~:text=An%20ontology%20%20is%20a,increasing%20compatibility%20between%20different%20graphs)). This ensures that data is structured consistently and that both humans and machines have a shared understanding of the data model. Ontologies are typically written in standardized languages such as OWL (Web Ontology Language) or RDFS, which allow rich semantic relationships (like class hierarchies, part-whole relationships, or logical restrictions) to be expressed. For example, an ontology for healthcare might define classes like *Patient*, *Doctor*, and *Hospital*, properties like *treats* (linking a Doctor to a Patient) and *employedBy* (linking a Doctor to a Hospital), and constraints such as “a Doctor can treat multiple Patients”. By using an ontology, an enterprise knowledge graph can enforce data integrity (e.g., a *Patient* node won’t erroneously link to another *Patient* with a *treats* relationship, since the ontology defines *treats* between Doctor and Patient) and enable semantic reasoning. In summary, an ontology provides a controlled vocabulary and a structure of the domain, acting as the “knowledge model” that the data in the graph adheres to ([What is an Ontology?](https://www.oxfordsemantic.tech/faqs/what-is-an-ontology#:~:text=An%20ontology%20%20is%20a,increasing%20compatibility%20between%20different%20graphs)). 

## Property Graph  
**Property Graph** is a common data model for graph databases in which nodes (vertices) and edges can both have associated properties (key-value pairs). In a property graph, data is represented as a set of vertices connected by edges, and each vertex or edge can carry a collection of properties that describe it ([What Are Property Graphs?](https://docs.oracle.com/en/database/oracle/property-graph/22.2/spgdg/what-are-property-graphs.html#:~:text=A%20property%20graph%20consists%20of,value%20pairs)). Edges in a property graph are typically directed and labeled, meaning an edge has a type (label) that describes the relationship (for example, an edge might have the label *PURCHASED* to link a *Customer* node to a *Product* node) ([What Are Property Graphs?](https://docs.oracle.com/en/database/oracle/property-graph/22.2/spgdg/what-are-property-graphs.html#:~:text=)). The key characteristics of a property graph are: (1) **Nodes** represent entities and can have properties (e.g., a Person node with properties like name="Alice", role="Engineer"), (2) **Edges** represent relationships between nodes and can also have properties (e.g., a KNOWS edge between two Person nodes might have a property `since=2020` indicating the friendship start year), and (3) **Edges have labels** indicating the type of relationship (KNOWS, PURCHASED, LOCATED_IN, etc.). This model is intuitive and flexible for many applications – it allows multi-faceted data to be stored in the graph without a fixed schema for every possible attribute. Property graphs are the foundation of many popular graph databases (such as Neo4j, Amazon Neptune (in Gremlin mode), and Apache JanusGraph), and queries on property graphs are executed using languages like Cypher, Gremlin, or the upcoming GQL standard. Compared to RDF graphs, property graphs do not require a global schema or ontology (though one can be used); they tend to be schema-optional and are optimized for operational graph queries and analytics. 

## Schema Evolution  
**Schema Evolution** refers to the process of adapting and changing a data schema or ontology over time without disrupting the existing system. In the context of knowledge graphs, schema evolution means updating the graph’s structure (classes, relationships, or constraints) as business requirements change, while preserving the integrity of the data. It involves adding, modifying, or deprecating elements of the schema in a controlled manner so that both old and new data remain accessible and consistent ([[PDF] Ontology Evolution: Not the Same as Schema Evolution - AMiner](https://static.aminer.org/pdf/PDF/001/003/468/ontology_evolution_not_the_same_as_schema_evolution.pdf#:~:text=Schema%20evolution%20is%20the%20ability,data%20through%20the%20new)). For example, an enterprise might extend its ontology with a new class *Vendor* and new relationships to accommodate a supplier management use case; schema evolution would ensure this can be done without breaking queries or data linked to the previous schema version. Key challenges in schema evolution include maintaining backward compatibility (applications depending on the old schema should still work) and migrating or transforming existing instance data to fit the new schema when necessary. Effective schema evolution practices often involve versioning the ontology, using tooling to map or transform data between schema versions, and communicating changes to all stakeholders. In summary, schema evolution enables a knowledge graph to grow and change gracefully as the knowledge domain itself evolves, ensuring longevity and flexibility of the system ([[PDF] Ontology Evolution: Not the Same as Schema Evolution - AMiner](https://static.aminer.org/pdf/PDF/001/003/468/ontology_evolution_not_the_same_as_schema_evolution.pdf#:~:text=Schema%20evolution%20is%20the%20ability,data%20through%20the%20new)). 

## Semantic Reasoning  
**Semantic Reasoning** (also known as inferencing) is the process of deriving new implicit knowledge from explicit data by applying logical rules and the semantic relationships defined in an ontology. In a knowledge graph, semantic reasoning uses the graph’s ontology and possibly additional rule sets to infer facts that are not directly stored as data but can be logically concluded ([An introduction to semantic technology and semantic reasoning - TextMine](https://textmine.com/post/an-introduction-to-semantic-technology-and-semantic-reasoning#:~:text=Semantic%20reasoning%20is%20a%20way,the%20body%20of%20a%20rule)). This capability allows the graph to answer questions with deeper insight. For example, if a knowledge graph knows that *London* isLocatedIn *United Kingdom*, and *United Kingdom* isPartOf *Europe*, a reasoner can infer that *London isPartOf Europe* even if that relationship wasn’t explicitly stored. Semantic reasoning engines (often based on description logic reasoners or rule-based systems) enforce consistency and enrich the data: they can deduce subclass relationships, compute transitive relationships, or apply domain-specific rules. *An example of a simple rule-based inference:* given a rule that if Person A is married to Person B, then Person B is also married to Person A, the reasoner can automatically add the inverse “married to” relationship ([An introduction to semantic technology and semantic reasoning - TextMine](https://textmine.com/post/an-introduction-to-semantic-technology-and-semantic-reasoning#:~:text=Semantic%20reasoning%20is%20a%20way,the%20body%20of%20a%20rule)). By leveraging semantic reasoning, enterprise knowledge graphs can uncover hidden connections and ensure that all implied knowledge is available for querying, without manually encoding every relationship. This leads to smarter applications – for instance, answering queries like “list all European capital cities” can return London because the reasoning engine knows London is a capital in the UK and the UK is in Europe, therefore London is a European capital. Semantic reasoning thus adds a layer of intelligence to the data, enabling more powerful and context-aware insights. 

## Shortest Path  
**Shortest Path** (in graph theory) is the problem of finding the path between two nodes that minimizes some distance metric, typically the number of edges or the total weight of the edges along the path. In an unweighted graph, a shortest path between two vertices is simply the path with the fewest hops (edges). In a weighted graph (where edges have weights or costs), the shortest path is the route that has the minimum sum of weights ([ShortestPath](https://www.cs.yale.edu/homes/aspnes/pinewiki/ShortestPath.html#:~:text=ShortestPath%20The%20shortest%20path%20problem,has%20the%20minimum%20total%20weight)). Shortest path algorithms (such as Dijkstra’s algorithm or BFS for unweighted graphs) are fundamental in graph analytics and are used to compute metrics like distances or to enable features like routing. *For example, in a road network graph, the shortest path algorithm can find the quickest route from one city to another, considering distance or travel time.* In a knowledge graph or social network, shortest path queries can help determine how closely related two entities are (e.g., “What is the shortest connection between Alice and Bob?” might reveal a chain of acquaintances). Finding a shortest path is crucial for applications like network optimization, recommendations (finding the “distance” between items or people), and understanding the degrees of separation in a network. Many graph query languages and analytics tools provide built-in support for shortest path computation due to its importance. 

## SPARQL  
**SPARQL** (SPARQL Protocol and RDF Query Language) is the standard query language for retrieving and manipulating data in Resource Description Framework (RDF) format. It is a powerful semantic query language designed specifically for graph data stored as RDF triples ([A Glossary of Knowledge Graph Terms - DataScienceCentral.com](https://www.datasciencecentral.com/a-glossary-of-knowledge-graph-terms/#:~:text=SPARQL)). With SPARQL, users can write queries to match patterns in the graph, much like SQL selects rows from tables – however, SPARQL’s patterns traverse the graph’s subject-predicate-object structure. For example, a SPARQL query can ask for all `?person` who `?person <worksFor> "ACME Corp"` and `?person <hasSkill> "Graph Databases"`, retrieving people who work at ACME Corp and have the skill “Graph Databases”. SPARQL can combine data across diverse datasets as long as they share common identifiers or ontology terms, which makes it ideal for integrating heterogeneous data in a knowledge graph ([A Glossary of Knowledge Graph Terms - DataScienceCentral.com](https://www.datasciencecentral.com/a-glossary-of-knowledge-graph-terms/#:~:text=SPARQL%20is%20a%20query%20language,of%20the%20triple%20store%20environment)). The language supports not only SELECT queries (to retrieve data) but also ASK (yes/no questions), CONSTRUCT (to build new RDF graphs from query results), and UPDATE operations. SPARQL 1.1, the current version (as of 2025), includes features like subqueries, aggregates, and federated querying, allowing queries to be distributed across multiple SPARQL endpoints. It plays a role analogous to SQL for relational databases, enabling complex graph queries with pattern matching, while respecting the semantics of the data ([A Glossary of Knowledge Graph Terms - DataScienceCentral.com](https://www.datasciencecentral.com/a-glossary-of-knowledge-graph-terms/#:~:text=SPARQL)). In enterprise knowledge graphs, SPARQL is commonly used to query triple stores and infer new insights by leveraging the rich relationships defined in ontologies. 

## Taxonomy  
**Taxonomy** in the context of knowledge management is a hierarchical classification system used to categorize and organize information into groups and sub-groups ([Taxonomies Versus Ontologies: A Short Guide - Fluree](https://flur.ee/fluree-blog/taxonomies-versus-ontologies-a-short-guide/#:~:text=What%20is%20a%20Taxonomy%3F)). It provides a controlled vocabulary of terms that are arranged in a tree-like structure, typically from broader categories down to more specific ones. Each node in a taxonomy represents a category, which can have child subcategories. For example, a company might maintain a product taxonomy: *Electronics* → *Computers* → *Laptops* would classify “Laptops” as a subset of “Computers,” which in turn is under the broad category “Electronics.” Taxonomies focus on **is-a** or **category-subcategory** relationships (simple hierarchies) and usually do not capture other types of relationships beyond the parent-child structure. In an enterprise knowledge graph, a taxonomy can be used to tag or classify content and data – for instance, tagging documents by topic, or categorizing customers by industry. This helps ensure consistency in how things are labeled and makes it easier to search and aggregate information. Taxonomies are often simpler than ontologies; they deal primarily with classification and don’t express complex rules. However, they are an important governance tool: a well-designed taxonomy improves data discoverability and clarity. Many organizations start by building a taxonomy (or reusing a standard one, like a geography or industry classification) as a foundation, and later extend it or map it into an ontology for more sophisticated semantic relationships. 

## Triple Store  
**Triple Store** (or RDF store) is a type of database optimized for storing and querying data modeled as RDF triples (subject-predicate-object statements). In a triple store, all information is represented as triples – for example, *(Alice – worksFor – ACMECorp)* or *(Paris – isCapitalOf – France)* are individual triples. The triple store is purpose-built to index and retrieve these triples efficiently via semantic query languages like SPARQL ([Triplestore | LINCS](https://lincsproject.ca/docs/terms/triplestore#:~:text=Triplestore)). Triple stores are a subset of NoSQL graph databases specifically tailored to the RDF data model; they often support reasoning and ontology-based queries. Key features of triple stores include the ability to store billions of triples, infer new triples through ontologies (if a reasoning engine is integrated), and perform SPARQL queries that can traverse the graph of triples. Unlike a property graph database, which might allow arbitrary properties on nodes and edges, a triple store maintains a minimalist data model: each triple links a subject to an object via a predicate, and additional context can be captured with quads (adding a named graph context) if needed. **Examples:** Popular triple store technologies include Apache Jena TDB/Fuseki, RDF4J, Blazegraph, GraphDB, Stardog, and Amazon Neptune (in RDF mode). These systems ensure that as the schema (ontology) evolves, existing data is still accessible – an aspect of **schema evolution**. A triple store enables data integration from various sources by using URIs for identifiers and linking data based on shared concepts (the essence of *Linked Data*). In summary, a triple store is the backbone of many semantic enterprise knowledge graphs, providing robust storage for interconnected facts and the ability to query them with rich semantic context ([Triplestore | LINCS](https://lincsproject.ca/docs/terms/triplestore#:~:text=Triplestore)). 

